{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89661c6-d089-4b23-b751-e345878af3cc",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fee386-ef48-4136-bc75-4b00b3b1a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import nfl_data_py as nfl\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import pandas as pd\n",
    "from imblearn.combine import SMOTEENN\n",
    "from DataPipelineFxn import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2148306c-f7ab-4994-990a-90738398e078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 done.\n",
      "2001 done.\n",
      "2002 done.\n",
      "2003 done.\n",
      "2004 done.\n",
      "2005 done.\n",
      "2006 done.\n",
      "2007 done.\n",
      "2008 done.\n",
      "2009 done.\n",
      "2010 done.\n",
      "2011 done.\n",
      "2012 done.\n",
      "2013 done.\n",
      "2014 done.\n",
      "2015 done.\n",
      "2016 done.\n",
      "2017 done.\n",
      "2018 done.\n",
      "2019 done.\n",
      "2020 done.\n",
      "2021 done.\n",
      "2022 done.\n",
      "2023 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/02 15:49:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/02 15:49:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/12/02 15:49:22 WARN TaskSetManager: Stage 0 contains a task of very large size (9437 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "years = []\n",
    "\n",
    "# loop through years from 2000 to 2023\n",
    "for i in range(2000,2024):\n",
    "    years.append(i)\n",
    "\n",
    "# removing game_date, removing time\n",
    "# play_type is the predictor col\n",
    "# columns to add: wind, temp, roof, qb_epa?, surface, \n",
    "cols = [\"home_team\", \"away_team\", \"season_type\", \"week\", \"posteam\", \"posteam_type\", \n",
    "        \"defteam\", \"side_of_field\", \"yardline_100\", \"quarter_seconds_remaining\", \n",
    "        \"half_seconds_remaining\", \"game_seconds_remaining\" , \"game_half\", \"down\", \n",
    "        \"drive\", \"qtr\",  \"ydstogo\", \"play_type\", \"posteam_timeouts_remaining\", \n",
    "        \"defteam_timeouts_remaining\", \"posteam_score\", \"defteam_score\", \"score_differential\", \n",
    "        \"ep\", \"epa\", \"season\", 'wind', 'temp', 'roof', 'surface']\n",
    "\n",
    "#TODO add weather to col\n",
    "\n",
    "data = nfl.import_pbp_data(years, downcast=False, cache=False, alt_path=None)\n",
    "\n",
    "# get the desired columns\n",
    "reduced_data = data.filter(items=cols) \n",
    "\n",
    "# select only where there are 4th downs\n",
    "forth_down = reduced_data.query(\"down==4.0\")\n",
    "\n",
    "# set up the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(forth_down)\n",
    "\n",
    "# remove nulls\n",
    "spark_df = spark_df.where(col(\"play_type\").isNotNull() & col('temp').isNotNull() & col('wind').isNotNull())\n",
    "\n",
    "# removing QB kneel\n",
    "spark_df = spark_df.where(col(\"play_type\") != \"qb_kneel\")\n",
    "\n",
    "# out sampling fractions\n",
    "fractions = {\n",
    "    \"field_goal\": 0.6,\n",
    "    \"no_play\": 0.6,\n",
    "    \"run\": 0.6,\n",
    "    \"punt\": 0.4,\n",
    "    \"pass\": 0.6\n",
    "}\n",
    "\n",
    "# get the sample\n",
    "train_df = spark_df.sampleBy(\"play_type\", fractions=fractions, seed=42)\n",
    "temp_mean = train_df.select(col('temp')).filter(~F.isnan('temp')).agg(F.mean('temp')).collect()[0][0]\n",
    "\n",
    "train_df = train_df.na.fill({'wind': 0, 'temp': temp_mean})\n",
    "\n",
    "# remove sample to form the test\n",
    "test_df = spark_df.subtract(train_df)\n",
    "\n",
    "str_col = [\"home_team\", \"away_team\", \"season_type\", \"posteam\", \"posteam_type\", \"defteam\", \"side_of_field\", \"game_half\",\n",
    "        \"play_type\", \"season\", 'roof', 'surface']\n",
    "str_col_output = [\"home_team_idx\", \"away_team_idx\", \"season_type_idx\", \"posteam_idx\", \"posteam_type_idx\", \"defteam_idx\",\n",
    "                  \"side_of_field_idx\", \"game_half_idx\", \"play_type_idx\", \"season_idx\", 'roof_idx', 'surface_idx']\n",
    "ohe_col_input = [\"home_team_idx\", \"away_team_idx\", \"season_type_idx\", \"posteam_idx\", \"posteam_type_idx\", \"defteam_idx\",\n",
    "                  \"side_of_field_idx\", \"game_half_idx\", \"season_idx\", 'roof_idx', 'surface_idx']\n",
    "ohe_col_vec = [\"home_team_vec\", \"away_team_vec\", \"season_type_vec\", \"posteam_vec\", \"posteam_type_vec\", \"defteam_vec\",\n",
    "                  \"side_of_field_vec\", \"game_half_ivec\", \"season_vec\", 'roof_vec', 'surface_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00df5564-abeb-448f-9087-9a15b6b460fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 15:46:33 WARN TaskSetManager: Stage 146 contains a task of very large size (9437 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "temp_mean = train_df.select(col('temp')).filter(~F.isnan('temp')).agg(F.mean('temp')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4a9645-e830-426e-9271-20fd2df70054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 15:49:29 WARN TaskSetManager: Stage 3 contains a task of very large size (9437 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert PySpark DataFrame to pandas DataFrame\n",
    "pandas_df = train_df.toPandas()\n",
    "\n",
    "# # find where wind and temp are not nan\n",
    "# wind_null_idx = pandas_df['wind'].isnull()\n",
    "# temp_null_idx = pandas_df['temp'].isnull()\n",
    "\n",
    "# # impute with 0 for wind and mean for temperature\n",
    "# # when dome, the wind and temp are NaN\n",
    "# pandas_df['wind'] = pandas_df['wind'].apply(lambda x: 0 if np.isnan(x) else x)\n",
    "# temp_mean = round(pandas_df['temp'].mean(), 2)\n",
    "# pandas_df['temo'] = pandas_df['temp'].apply(lambda x: temp_mean if np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9053f46c-3411-4a3d-b529-b02d927fec22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 15:49:34 WARN TaskSetManager: Stage 4 contains a task of very large size (3174 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/02 15:49:37 WARN TaskSetManager: Stage 5 contains a task of very large size (1934 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Undersample the data\n",
    "rus = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(pandas_df.drop(\"play_type\", axis=1), pandas_df[\"play_type\"])\n",
    "\n",
    "pandas_df.drop(\"play_type\", axis=1, inplace=True)\n",
    "\n",
    "# Convert back to PySpark DataFrame\n",
    "undersampled_df = spark.createDataFrame(pd.DataFrame(X_resampled, columns=pandas_df.columns).assign(play_type=y_resampled))\n",
    "\n",
    "# Convert PySpark DataFrame to pandas DataFrame\n",
    "pandas_df = undersampled_df.toPandas()\n",
    "\n",
    "# Undersample the data\n",
    "rus2 = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = rus2.fit_resample(pandas_df.drop(\"play_type\", axis=1), pandas_df[\"play_type\"])\n",
    "\n",
    "pandas_df.drop(\"play_type\", axis=1, inplace=True)\n",
    "\n",
    "# Convert back to PySpark DataFrame\n",
    "undersampled_df2 = spark.createDataFrame(pd.DataFrame(X_resampled, columns=pandas_df.columns).assign(play_type=y_resampled))\n",
    "\n",
    "str_col2 = [\"home_team\", \"away_team\", \"season_type\", \"posteam\", \"posteam_type\", \"defteam\", \"side_of_field\", \"game_half\", \"season\", 'roof', 'surface']\n",
    "\n",
    "# Convert PySpark DataFrame to pandas DataFrame\n",
    "pandas_df = undersampled_df2.toPandas()\n",
    "pandas_df2 = pandas_df.copy()\n",
    "pandas_df2.drop(\"play_type\", axis=1, inplace=True)\n",
    "\n",
    "categorical_indices = [pandas_df2.columns.get_loc(col) for col in str_col2]\n",
    "\n",
    "# Define SMOTENC with indices of categorical columns\n",
    "smote_enn = SMOTENC(categorical_features=categorical_indices, random_state=42)\n",
    "#X_resampled, y_resampled = rus.fit_resample(pandas_df.drop(\"play_type\", axis=1), pandas_df[\"play_type\"])\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(pandas_df.drop(\"play_type\", axis=1), pandas_df[\"play_type\"])\n",
    "pandas_df.drop(\"play_type\", axis=1, inplace=True)\n",
    "\n",
    "# Convert back to PySpark DataFrame\n",
    "train_df = spark.createDataFrame(pd.DataFrame(X_resampled, columns=pandas_df.columns).assign(play_type=y_resampled))\n",
    "\n",
    "# # account for wind and temp nans\n",
    "# train_df = train_df.withColumn(\n",
    "#     'wind',\n",
    "#     F.when(F.isnull(train_df.wind), 0).otherwise(train_df.wind)\n",
    "# )\n",
    "\n",
    "# temp_mean = train_df.select('temp').agg(F.mean()).collect()[0][0]\n",
    "\n",
    "# train_df = train_df.withColumn(\n",
    "#     'temp',\n",
    "#     F.when(F.isnull(train_df.temp), temp_mean).otherwise(train_df.wind)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0cef91-be51-42ee-9e1f-f39768e54c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536d5c12-5c58-4d62-9a06-3aab6838a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df['wind'] = pandas_df['wind'].apply(lambda x: 0 if np.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0074d0fd-7fd7-412d-bf5d-1e8f98663468",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_901430/623452267.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_resampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnum_nans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_resampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_nans\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' is nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/gpfs/tardis/home/ean8fr/distributed-computing-final/env/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5898\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5899\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "for colname in X_resampled.columns:\n",
    "    num_nans = X_resampled.select(col(colname)).filter(F.isnull(col(colname))).count()\n",
    "    if num_nans > 0:\n",
    "        print(colnames, ' is nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91c3241d-c460-4ab5-8292-450e9ba46762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home_team                        0\n",
       "away_team                        0\n",
       "season_type                      0\n",
       "week                             0\n",
       "posteam                          0\n",
       "posteam_type                     0\n",
       "defteam                          0\n",
       "side_of_field                    0\n",
       "yardline_100                     0\n",
       "quarter_seconds_remaining        0\n",
       "half_seconds_remaining           0\n",
       "game_seconds_remaining           0\n",
       "game_half                        0\n",
       "down                             0\n",
       "drive                            0\n",
       "qtr                              0\n",
       "ydstogo                          0\n",
       "posteam_timeouts_remaining       0\n",
       "defteam_timeouts_remaining       0\n",
       "posteam_score                    0\n",
       "defteam_score                    0\n",
       "score_differential               0\n",
       "ep                               0\n",
       "epa                              0\n",
       "season                           0\n",
       "wind                          4627\n",
       "temp                          4627\n",
       "roof                             0\n",
       "surface                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b206e89-b75b-41f0-b8fc-d1be80239797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['home_team',\n",
       " 'away_team',\n",
       " 'season_type',\n",
       " 'week',\n",
       " 'posteam',\n",
       " 'posteam_type',\n",
       " 'defteam',\n",
       " 'side_of_field',\n",
       " 'yardline_100',\n",
       " 'quarter_seconds_remaining',\n",
       " 'half_seconds_remaining',\n",
       " 'game_seconds_remaining',\n",
       " 'game_half',\n",
       " 'down',\n",
       " 'drive',\n",
       " 'qtr',\n",
       " 'ydstogo',\n",
       " 'play_type',\n",
       " 'posteam_timeouts_remaining',\n",
       " 'defteam_timeouts_remaining',\n",
       " 'posteam_score',\n",
       " 'defteam_score',\n",
       " 'score_differential',\n",
       " 'ep',\n",
       " 'epa',\n",
       " 'season',\n",
       " 'wind',\n",
       " 'temp',\n",
       " 'roof',\n",
       " 'surface']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe674439-834d-443e-8568-a1b8d86b174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 14:44:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/12/02 14:44:29 WARN TaskSetManager: Stage 0 contains a task of very large size (9437 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/02 14:44:40 WARN TaskSetManager: Stage 1 contains a task of very large size (2445 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/02 14:44:42 WARN TaskSetManager: Stage 2 contains a task of very large size (1453 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Convert PySpark DataFrame to pandas DataFrame\n",
    "pandas_df = train_df.toPandas()\n",
    "\n",
    "# find where wind and temp are not nan\n",
    "wind_idx = ~pandas_df['wind'].isnull()\n",
    "temp_idx = ~pandas_df['temp'].isnull()\n",
    "\n",
    "# sample down\n",
    "pandas_df = pandas_df[wind_idx & temp_idx]\n",
    "\n",
    "# Undersample the data\n",
    "rus = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(pandas_df.drop(\"play_type\", axis=1), pandas_df[\"play_type\"])\n",
    "\n",
    "pandas_df.drop(\"play_type\", axis=1, inplace=True)\n",
    "\n",
    "# Convert back to PySpark DataFrame\n",
    "undersampled_df = spark.createDataFrame(pd.DataFrame(X_resampled, columns=pandas_df.columns).assign(play_type=y_resampled))\n",
    "\n",
    "# Convert PySpark DataFrame to pandas DataFrame\n",
    "pandas_df = undersampled_df.toPandas()\n",
    "\n",
    "# Undersample the data\n",
    "rus2 = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = rus2.fit_resample(pandas_df.drop(\"play_type\", axis=1), pandas_df[\"play_type\"])\n",
    "\n",
    "pandas_df.drop(\"play_type\", axis=1, inplace=True)\n",
    "\n",
    "# Convert back to PySpark DataFrame\n",
    "undersampled_df2 = spark.createDataFrame(pd.DataFrame(X_resampled, columns=pandas_df.columns).assign(play_type=y_resampled))\n",
    "\n",
    "str_col2 = [\"home_team\", \"away_team\", \"season_type\", \"posteam\", \"posteam_type\", \"defteam\", \"side_of_field\", \"game_half\", \"season\", 'roof', 'surface']\n",
    "\n",
    "# Convert PySpark DataFrame to pandas DataFrame\n",
    "pandas_df = undersampled_df2.toPandas()\n",
    "pandas_df2 = pandas_df.copy()\n",
    "pandas_df2.drop(\"play_type\", axis=1, inplace=True)\n",
    "\n",
    "categorical_indices = [pandas_df2.columns.get_loc(col) for col in str_col2]\n",
    "\n",
    "# Define SMOTENC with indices of categorical columns\n",
    "smote_enn = SMOTENC(categorical_features=categorical_indices, random_state=42)\n",
    "#X_resampled, y_resampled = rus.fit_resample(pandas_df.drop(\"play_type\", axis=1), pandas_df[\"play_type\"])\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(pandas_df.drop(\"play_type\", axis=1), pandas_df[\"play_type\"])\n",
    "pandas_df.drop(\"play_type\", axis=1, inplace=True)\n",
    "\n",
    "# Convert back to PySpark DataFrame\n",
    "train_df = spark.createDataFrame(pd.DataFrame(X_resampled, columns=pandas_df.columns).assign(play_type=y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5add963-64ac-4e6b-abda-16638730d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 14:46:52 WARN TaskSetManager: Stage 9 contains a task of very large size (1745 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "| play_type|count|\n",
      "+----------+-----+\n",
      "|field_goal| 3573|\n",
      "|   no_play| 3573|\n",
      "|      punt| 3573|\n",
      "|      pass| 3573|\n",
      "|       run| 3573|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 14:49:41 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-09c5991d-1d9b-4caf-ac5a-171c39f8971a. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-09c5991d-1d9b-4caf-ac5a-171c39f8971a\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/02 14:49:42 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-09c5991d-1d9b-4caf-ac5a-171c39f8971a/2b. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-09c5991d-1d9b-4caf-ac5a-171c39f8971a/2b\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/02 14:49:43 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-09c5991d-1d9b-4caf-ac5a-171c39f8971a/39. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-09c5991d-1d9b-4caf-ac5a-171c39f8971a/39\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2052)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "train_df.groupby(train_df.play_type).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e582242-2514-4702-8eae-e45898f3a21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b2d9e6-bc5c-4b44-bc34-a40d6e9e6056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 done.\n",
      "2001 done.\n",
      "2002 done.\n",
      "2003 done.\n",
      "2004 done.\n",
      "2005 done.\n",
      "2006 done.\n",
      "2007 done.\n",
      "2008 done.\n",
      "2009 done.\n",
      "2010 done.\n",
      "2011 done.\n",
      "2012 done.\n",
      "2013 done.\n",
      "2014 done.\n",
      "2015 done.\n",
      "2016 done.\n",
      "2017 done.\n",
      "2018 done.\n",
      "2019 done.\n",
      "2020 done.\n",
      "2021 done.\n",
      "2022 done.\n",
      "2023 done.\n"
     ]
    }
   ],
   "source": [
    "years = []\n",
    "    \n",
    "# loop through years from 2000 to 2023\n",
    "for i in range(2000,2024):\n",
    "    years.append(i)\n",
    "\n",
    "data = nfl.import_pbp_data(years, downcast=False, cache=False, alt_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b74caa-7b9f-4c86-99ef-825faf064bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "down_4th = data.query(\"down==4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a24bfac-6245-4627-b3b6-f10a132c108c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.63"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(down_4th['temp'].mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43f3c5-9c51-4816-8d91-48d01074c78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386b852d-96b6-4cd4-8c92-c2d414a16904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play_id\n",
      "game_id\n",
      "old_game_id\n",
      "home_team\n",
      "away_team\n",
      "season_type\n",
      "week\n",
      "posteam\n",
      "posteam_type\n",
      "defteam\n",
      "side_of_field\n",
      "yardline_100\n",
      "game_date\n",
      "quarter_seconds_remaining\n",
      "half_seconds_remaining\n",
      "game_seconds_remaining\n",
      "game_half\n",
      "quarter_end\n",
      "drive\n",
      "sp\n",
      "qtr\n",
      "down\n",
      "goal_to_go\n",
      "time\n",
      "yrdln\n",
      "ydstogo\n",
      "ydsnet\n",
      "desc\n",
      "play_type\n",
      "yards_gained\n",
      "shotgun\n",
      "no_huddle\n",
      "qb_dropback\n",
      "qb_kneel\n",
      "qb_spike\n",
      "qb_scramble\n",
      "pass_length\n",
      "pass_location\n",
      "air_yards\n",
      "yards_after_catch\n",
      "run_location\n",
      "run_gap\n",
      "field_goal_result\n",
      "kick_distance\n",
      "extra_point_result\n",
      "two_point_conv_result\n",
      "home_timeouts_remaining\n",
      "away_timeouts_remaining\n",
      "timeout\n",
      "timeout_team\n",
      "td_team\n",
      "td_player_name\n",
      "td_player_id\n",
      "posteam_timeouts_remaining\n",
      "defteam_timeouts_remaining\n",
      "total_home_score\n",
      "total_away_score\n",
      "posteam_score\n",
      "defteam_score\n",
      "score_differential\n",
      "posteam_score_post\n",
      "defteam_score_post\n",
      "score_differential_post\n",
      "no_score_prob\n",
      "opp_fg_prob\n",
      "opp_safety_prob\n",
      "opp_td_prob\n",
      "fg_prob\n",
      "safety_prob\n",
      "td_prob\n",
      "extra_point_prob\n",
      "two_point_conversion_prob\n",
      "ep\n",
      "epa\n",
      "total_home_epa\n",
      "total_away_epa\n",
      "total_home_rush_epa\n",
      "total_away_rush_epa\n",
      "total_home_pass_epa\n",
      "total_away_pass_epa\n",
      "air_epa\n",
      "yac_epa\n",
      "comp_air_epa\n",
      "comp_yac_epa\n",
      "total_home_comp_air_epa\n",
      "total_away_comp_air_epa\n",
      "total_home_comp_yac_epa\n",
      "total_away_comp_yac_epa\n",
      "total_home_raw_air_epa\n",
      "total_away_raw_air_epa\n",
      "total_home_raw_yac_epa\n",
      "total_away_raw_yac_epa\n",
      "wp\n",
      "def_wp\n",
      "home_wp\n",
      "away_wp\n",
      "wpa\n",
      "vegas_wpa\n",
      "vegas_home_wpa\n",
      "home_wp_post\n",
      "away_wp_post\n",
      "vegas_wp\n",
      "vegas_home_wp\n",
      "total_home_rush_wpa\n",
      "total_away_rush_wpa\n",
      "total_home_pass_wpa\n",
      "total_away_pass_wpa\n",
      "air_wpa\n",
      "yac_wpa\n",
      "comp_air_wpa\n",
      "comp_yac_wpa\n",
      "total_home_comp_air_wpa\n",
      "total_away_comp_air_wpa\n",
      "total_home_comp_yac_wpa\n",
      "total_away_comp_yac_wpa\n",
      "total_home_raw_air_wpa\n",
      "total_away_raw_air_wpa\n",
      "total_home_raw_yac_wpa\n",
      "total_away_raw_yac_wpa\n",
      "punt_blocked\n",
      "first_down_rush\n",
      "first_down_pass\n",
      "first_down_penalty\n",
      "third_down_converted\n",
      "third_down_failed\n",
      "fourth_down_converted\n",
      "fourth_down_failed\n",
      "incomplete_pass\n",
      "touchback\n",
      "interception\n",
      "punt_inside_twenty\n",
      "punt_in_endzone\n",
      "punt_out_of_bounds\n",
      "punt_downed\n",
      "punt_fair_catch\n",
      "kickoff_inside_twenty\n",
      "kickoff_in_endzone\n",
      "kickoff_out_of_bounds\n",
      "kickoff_downed\n",
      "kickoff_fair_catch\n",
      "fumble_forced\n",
      "fumble_not_forced\n",
      "fumble_out_of_bounds\n",
      "solo_tackle\n",
      "safety\n",
      "penalty\n",
      "tackled_for_loss\n",
      "fumble_lost\n",
      "own_kickoff_recovery\n",
      "own_kickoff_recovery_td\n",
      "qb_hit\n",
      "rush_attempt\n",
      "pass_attempt\n",
      "sack\n",
      "touchdown\n",
      "pass_touchdown\n",
      "rush_touchdown\n",
      "return_touchdown\n",
      "extra_point_attempt\n",
      "two_point_attempt\n",
      "field_goal_attempt\n",
      "kickoff_attempt\n",
      "punt_attempt\n",
      "fumble\n",
      "complete_pass\n",
      "assist_tackle\n",
      "lateral_reception\n",
      "lateral_rush\n",
      "lateral_return\n",
      "lateral_recovery\n",
      "passer_player_id\n",
      "passer_player_name\n",
      "passing_yards\n",
      "receiver_player_id\n",
      "receiver_player_name\n",
      "receiving_yards\n",
      "rusher_player_id\n",
      "rusher_player_name\n",
      "rushing_yards\n",
      "lateral_receiver_player_id\n",
      "lateral_receiver_player_name\n",
      "lateral_receiving_yards\n",
      "lateral_rusher_player_id\n",
      "lateral_rusher_player_name\n",
      "lateral_rushing_yards\n",
      "lateral_sack_player_id\n",
      "lateral_sack_player_name\n",
      "interception_player_id\n",
      "interception_player_name\n",
      "lateral_interception_player_id\n",
      "lateral_interception_player_name\n",
      "punt_returner_player_id\n",
      "punt_returner_player_name\n",
      "lateral_punt_returner_player_id\n",
      "lateral_punt_returner_player_name\n",
      "kickoff_returner_player_name\n",
      "kickoff_returner_player_id\n",
      "lateral_kickoff_returner_player_id\n",
      "lateral_kickoff_returner_player_name\n",
      "punter_player_id\n",
      "punter_player_name\n",
      "kicker_player_name\n",
      "kicker_player_id\n",
      "own_kickoff_recovery_player_id\n",
      "own_kickoff_recovery_player_name\n",
      "blocked_player_id\n",
      "blocked_player_name\n",
      "tackle_for_loss_1_player_id\n",
      "tackle_for_loss_1_player_name\n",
      "tackle_for_loss_2_player_id\n",
      "tackle_for_loss_2_player_name\n",
      "qb_hit_1_player_id\n",
      "qb_hit_1_player_name\n",
      "qb_hit_2_player_id\n",
      "qb_hit_2_player_name\n",
      "forced_fumble_player_1_team\n",
      "forced_fumble_player_1_player_id\n",
      "forced_fumble_player_1_player_name\n",
      "forced_fumble_player_2_team\n",
      "forced_fumble_player_2_player_id\n",
      "forced_fumble_player_2_player_name\n",
      "solo_tackle_1_team\n",
      "solo_tackle_2_team\n",
      "solo_tackle_1_player_id\n",
      "solo_tackle_2_player_id\n",
      "solo_tackle_1_player_name\n",
      "solo_tackle_2_player_name\n",
      "assist_tackle_1_player_id\n",
      "assist_tackle_1_player_name\n",
      "assist_tackle_1_team\n",
      "assist_tackle_2_player_id\n",
      "assist_tackle_2_player_name\n",
      "assist_tackle_2_team\n",
      "assist_tackle_3_player_id\n",
      "assist_tackle_3_player_name\n",
      "assist_tackle_3_team\n",
      "assist_tackle_4_player_id\n",
      "assist_tackle_4_player_name\n",
      "assist_tackle_4_team\n",
      "tackle_with_assist\n",
      "tackle_with_assist_1_player_id\n",
      "tackle_with_assist_1_player_name\n",
      "tackle_with_assist_1_team\n",
      "tackle_with_assist_2_player_id\n",
      "tackle_with_assist_2_player_name\n",
      "tackle_with_assist_2_team\n",
      "pass_defense_1_player_id\n",
      "pass_defense_1_player_name\n",
      "pass_defense_2_player_id\n",
      "pass_defense_2_player_name\n",
      "fumbled_1_team\n",
      "fumbled_1_player_id\n",
      "fumbled_1_player_name\n",
      "fumbled_2_player_id\n",
      "fumbled_2_player_name\n",
      "fumbled_2_team\n",
      "fumble_recovery_1_team\n",
      "fumble_recovery_1_yards\n",
      "fumble_recovery_1_player_id\n",
      "fumble_recovery_1_player_name\n",
      "fumble_recovery_2_team\n",
      "fumble_recovery_2_yards\n",
      "fumble_recovery_2_player_id\n",
      "fumble_recovery_2_player_name\n",
      "sack_player_id\n",
      "sack_player_name\n",
      "half_sack_1_player_id\n",
      "half_sack_1_player_name\n",
      "half_sack_2_player_id\n",
      "half_sack_2_player_name\n",
      "return_team\n",
      "return_yards\n",
      "penalty_team\n",
      "penalty_player_id\n",
      "penalty_player_name\n",
      "penalty_yards\n",
      "replay_or_challenge\n",
      "replay_or_challenge_result\n",
      "penalty_type\n",
      "defensive_two_point_attempt\n",
      "defensive_two_point_conv\n",
      "defensive_extra_point_attempt\n",
      "defensive_extra_point_conv\n",
      "safety_player_name\n",
      "safety_player_id\n",
      "season\n",
      "cp\n",
      "cpoe\n",
      "series\n",
      "series_success\n",
      "series_result\n",
      "order_sequence\n",
      "start_time\n",
      "time_of_day\n",
      "stadium\n",
      "weather\n",
      "nfl_api_id\n",
      "play_clock\n",
      "play_deleted\n",
      "play_type_nfl\n",
      "special_teams_play\n",
      "st_play_type\n",
      "end_clock_time\n",
      "end_yard_line\n",
      "fixed_drive\n",
      "fixed_drive_result\n",
      "drive_real_start_time\n",
      "drive_play_count\n",
      "drive_time_of_possession\n",
      "drive_first_downs\n",
      "drive_inside20\n",
      "drive_ended_with_score\n",
      "drive_quarter_start\n",
      "drive_quarter_end\n",
      "drive_yards_penalized\n",
      "drive_start_transition\n",
      "drive_end_transition\n",
      "drive_game_clock_start\n",
      "drive_game_clock_end\n",
      "drive_start_yard_line\n",
      "drive_end_yard_line\n",
      "drive_play_id_started\n",
      "drive_play_id_ended\n",
      "away_score\n",
      "home_score\n",
      "location\n",
      "result\n",
      "total\n",
      "spread_line\n",
      "total_line\n",
      "div_game\n",
      "roof\n",
      "surface\n",
      "temp\n",
      "wind\n",
      "home_coach\n",
      "away_coach\n",
      "stadium_id\n",
      "game_stadium\n",
      "aborted_play\n",
      "success\n",
      "passer\n",
      "passer_jersey_number\n",
      "rusher\n",
      "rusher_jersey_number\n",
      "receiver\n",
      "receiver_jersey_number\n",
      "pass\n",
      "rush\n",
      "first_down\n",
      "special\n",
      "play\n",
      "passer_id\n",
      "rusher_id\n",
      "receiver_id\n",
      "name\n",
      "jersey_number\n",
      "id\n",
      "fantasy_player_name\n",
      "fantasy_player_id\n",
      "fantasy\n",
      "fantasy_id\n",
      "out_of_bounds\n",
      "home_opening_kickoff\n",
      "qb_epa\n",
      "xyac_epa\n",
      "xyac_mean_yardage\n",
      "xyac_median_yardage\n",
      "xyac_success\n",
      "xyac_fd\n",
      "xpass\n",
      "pass_oe\n",
      "old_game_id_x\n",
      "nflverse_game_id\n",
      "old_game_id_y\n",
      "possession_team\n",
      "offense_formation\n",
      "offense_personnel\n",
      "defenders_in_box\n",
      "defense_personnel\n",
      "number_of_pass_rushers\n",
      "players_on_play\n",
      "offense_players\n",
      "defense_players\n",
      "n_offense\n",
      "n_defense\n",
      "ngs_air_yards\n",
      "time_to_throw\n",
      "was_pressure\n",
      "route\n",
      "defense_man_zone_type\n",
      "defense_coverage_type\n"
     ]
    }
   ],
   "source": [
    "for i in data.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69529c38-7147-4ec3-bab9-aa2a3573e094",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurface\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data['surface'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13526a-f7bc-480e-91fb-735de2020e60",
   "metadata": {},
   "source": [
    "### Create the Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1031091b-16bb-40b9-986e-851fc68d55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6fec1e-7a73-41b2-9dab-e9cd3a5f2f11",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '[' (3723565909.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    pipeline = Pipeline(stages=[stringIndexer, ohe, va, scaler, mlp)\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '['\n"
     ]
    }
   ],
   "source": [
    "str_col = [\"home_team\", \"away_team\", \"season_type\", \"posteam\", \"posteam_type\", \"defteam\", \"side_of_field\", \"game_half\",\n",
    "        \"play_type\", \"season\", 'roof', 'surface']\n",
    "str_col_output = [\"home_team_idx\", \"away_team_idx\", \"season_type_idx\", \"posteam_idx\", \"posteam_type_idx\", \"defteam_idx\",\n",
    "                  \"side_of_field_idx\", \"game_half_idx\", \"play_type_idx\", \"season_idx\", 'roof_idx', 'surface_idx']\n",
    "ohe_col_input = [\"home_team_idx\", \"away_team_idx\", \"season_type_idx\", \"posteam_idx\", \"posteam_type_idx\", \"defteam_idx\",\n",
    "                  \"side_of_field_idx\", \"game_half_idx\", \"season_idx\", 'roof_idx', 'surface_idx']\n",
    "ohe_col_vec = [\"home_team_vec\", \"away_team_vec\", \"season_type_vec\", \"posteam_vec\", \"posteam_type_vec\", \"defteam_vec\",\n",
    "                  \"side_of_field_vec\", \"game_half_ivec\", \"season_vec\", 'roof_vec', 'surface_vec']\n",
    "\n",
    "\n",
    "# process using string indexer first for catgeorical features  \n",
    "stringIndexer = StringIndexer(inputCols=str_col, outputCols=str_col_output)\n",
    "\n",
    "# process rating data into second feature\n",
    "ohe = OneHotEncoder(inputCols=ohe_col_input, outputCols=ohe_col_vec) \n",
    "\n",
    "# Assemble features column\n",
    "va = VectorAssembler(inputCols=ohe_col_vec, outputCol=\"features\") \n",
    "\n",
    "# process data using maxabs scaler, not necessarily important for trees but consistency\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# define the model\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol = 'features', labelCol = 'play_type_idx', layers = [, 5],\n",
    "    solver = 'gd', seed = 2002\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline = Pipeline(stages=[stringIndexer, ohe, va, scaler, mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distributed-computing",
   "language": "python",
   "name": "distributed-computing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
