{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc445894-a5bd-4768-8672-bcebea5aebfa",
   "metadata": {},
   "source": [
    "### Creating Model 3\n",
    "\n",
    "* Creating a 3rd model for the NFL Play data. The goal is to predict the play type on 4th down.\n",
    "* Logistic Regression & Random Forest have been completed, so the next step is gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e1acab1-8e72-4add-9c2c-1798b8967ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import nfl_data_py as nfl\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import pandas as pd\n",
    "from imblearn.combine import SMOTEENN\n",
    "from DataPipelineFxn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28b6116-fab9-4cfd-be1c-8000d1a6292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 done.\n",
      "2001 done.\n",
      "2002 done.\n",
      "2003 done.\n",
      "2004 done.\n",
      "2005 done.\n",
      "2006 done.\n",
      "2007 done.\n",
      "2008 done.\n",
      "2009 done.\n",
      "2010 done.\n",
      "2011 done.\n",
      "2012 done.\n",
      "2013 done.\n",
      "2014 done.\n",
      "2015 done.\n",
      "2016 done.\n",
      "2017 done.\n",
      "2018 done.\n",
      "2019 done.\n",
      "2020 done.\n",
      "2021 done.\n",
      "2022 done.\n",
      "2023 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/01 19:29:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/01 19:29:43 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/12/01 19:29:43 WARN TaskSetManager: Stage 0 contains a task of very large size (8310 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/01 19:29:49 WARN TaskSetManager: Stage 1 contains a task of very large size (2683 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/01 19:29:52 WARN TaskSetManager: Stage 2 contains a task of very large size (1631 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# get the spark session and data frame\n",
    "spark, df = GetSparkDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3913e0-f360-4edd-bd94-9bee88e15653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['home_team',\n",
       " 'away_team',\n",
       " 'season_type',\n",
       " 'week',\n",
       " 'posteam',\n",
       " 'posteam_type',\n",
       " 'defteam',\n",
       " 'side_of_field',\n",
       " 'yardline_100',\n",
       " 'quarter_seconds_remaining',\n",
       " 'half_seconds_remaining',\n",
       " 'game_seconds_remaining',\n",
       " 'game_half',\n",
       " 'down',\n",
       " 'drive',\n",
       " 'qtr',\n",
       " 'ydstogo',\n",
       " 'posteam_timeouts_remaining',\n",
       " 'defteam_timeouts_remaining',\n",
       " 'posteam_score',\n",
       " 'defteam_score',\n",
       " 'score_differential',\n",
       " 'ep',\n",
       " 'epa',\n",
       " 'season',\n",
       " 'play_type']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968cfd5-3a1e-4df9-a459-a18f88dfebb4",
   "metadata": {},
   "source": [
    "### Create the Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84eaa2e8-7fb1-478e-86eb-ee6fec29cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0fd6a-0322-4cb1-ba56-ab40c1f574de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process using string indexer first for catgeorical features  \n",
    "stringIndexer = StringIndexer(inputCols=str_col, outputCols=str_col_output)\n",
    "\n",
    "# process rating data into second feature\n",
    "ohe = OneHotEncoder(inputCols=ohe_col_input, outputCols=ohe_col_vec) \n",
    "\n",
    "# Assemble features column\n",
    "va = VectorAssembler(inputCols=ohe_col_vec, outputCol=\"features\") \n",
    "\n",
    "# process data using maxabs scaler, not necessarily important for trees but consistency\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# define the model\n",
    "gbt = GBTClassifier(\n",
    "    inputCol = 'features', labelCol = 'play_type'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23916e45-b255-4d3d-81a7-6ea8711d2e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GBTClassifier in module pyspark.ml.classification:\n",
      "\n",
      "class GBTClassifier(_JavaProbabilisticClassifier, _GBTClassifierParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n",
      " |  GBTClassifier(*, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, lossType: str = 'logistic', maxIter: int = 20, stepSize: float = 0.1, seed: Optional[int] = None, subsamplingRate: float = 1.0, impurity: str = 'variance', featureSubsetStrategy: str = 'all', validationTol: float = 0.01, validationIndicatorCol: Optional[str] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0, weightCol: Optional[str] = None)\n",
      " |  \n",
      " |  `Gradient-Boosted Trees (GBTs) <http://en.wikipedia.org/wiki/Gradient_boosting>`_\n",
      " |  learning algorithm for classification.\n",
      " |  It supports binary labels, as well as both continuous and categorical features.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Multiclass labels are not currently supported.\n",
      " |  \n",
      " |  The implementation is based upon: J.H. Friedman. \"Stochastic Gradient Boosting.\" 1999.\n",
      " |  \n",
      " |  Gradient Boosting vs. TreeBoost:\n",
      " |  \n",
      " |  - This implementation is for Stochastic Gradient Boosting, not for TreeBoost.\n",
      " |  - Both algorithms learn tree ensembles by minimizing loss functions.\n",
      " |  - TreeBoost (Friedman, 1999) additionally modifies the outputs at tree leaf nodes\n",
      " |    based on the loss function, whereas the original gradient boosting method does not.\n",
      " |  - We expect to implement TreeBoost in the future:\n",
      " |    `SPARK-4240 <https://issues.apache.org/jira/browse/SPARK-4240>`_\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from numpy import allclose\n",
      " |  >>> from pyspark.ml.linalg import Vectors\n",
      " |  >>> from pyspark.ml.feature import StringIndexer\n",
      " |  >>> df = spark.createDataFrame([\n",
      " |  ...     (1.0, Vectors.dense(1.0)),\n",
      " |  ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n",
      " |  >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
      " |  >>> si_model = stringIndexer.fit(df)\n",
      " |  >>> td = si_model.transform(df)\n",
      " |  >>> gbt = GBTClassifier(maxIter=5, maxDepth=2, labelCol=\"indexed\", seed=42,\n",
      " |  ...     leafCol=\"leafId\")\n",
      " |  >>> gbt.setMaxIter(5)\n",
      " |  GBTClassifier...\n",
      " |  >>> gbt.setMinWeightFractionPerNode(0.049)\n",
      " |  GBTClassifier...\n",
      " |  >>> gbt.getMaxIter()\n",
      " |  5\n",
      " |  >>> gbt.getFeatureSubsetStrategy()\n",
      " |  'all'\n",
      " |  >>> model = gbt.fit(td)\n",
      " |  >>> model.getLabelCol()\n",
      " |  'indexed'\n",
      " |  >>> model.setFeaturesCol(\"features\")\n",
      " |  GBTClassificationModel...\n",
      " |  >>> model.setThresholds([0.3, 0.7])\n",
      " |  GBTClassificationModel...\n",
      " |  >>> model.getThresholds()\n",
      " |  [0.3, 0.7]\n",
      " |  >>> model.featureImportances\n",
      " |  SparseVector(1, {0: 1.0})\n",
      " |  >>> allclose(model.treeWeights, [1.0, 0.1, 0.1, 0.1, 0.1])\n",
      " |  True\n",
      " |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n",
      " |  >>> model.predict(test0.head().features)\n",
      " |  0.0\n",
      " |  >>> model.predictRaw(test0.head().features)\n",
      " |  DenseVector([1.1697, -1.1697])\n",
      " |  >>> model.predictProbability(test0.head().features)\n",
      " |  DenseVector([0.9121, 0.0879])\n",
      " |  >>> result = model.transform(test0).head()\n",
      " |  >>> result.prediction\n",
      " |  0.0\n",
      " |  >>> result.leafId\n",
      " |  DenseVector([0.0, 0.0, 0.0, 0.0, 0.0])\n",
      " |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
      " |  >>> model.transform(test1).head().prediction\n",
      " |  1.0\n",
      " |  >>> model.totalNumNodes\n",
      " |  15\n",
      " |  >>> print(model.toDebugString)\n",
      " |  GBTClassificationModel...numTrees=5...\n",
      " |  >>> gbtc_path = temp_path + \"gbtc\"\n",
      " |  >>> gbt.save(gbtc_path)\n",
      " |  >>> gbt2 = GBTClassifier.load(gbtc_path)\n",
      " |  >>> gbt2.getMaxDepth()\n",
      " |  2\n",
      " |  >>> model_path = temp_path + \"gbtc_model\"\n",
      " |  >>> model.save(model_path)\n",
      " |  >>> model2 = GBTClassificationModel.load(model_path)\n",
      " |  >>> model.featureImportances == model2.featureImportances\n",
      " |  True\n",
      " |  >>> model.treeWeights == model2.treeWeights\n",
      " |  True\n",
      " |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
      " |  True\n",
      " |  >>> model.trees\n",
      " |  [DecisionTreeRegressionModel...depth=..., DecisionTreeRegressionModel...]\n",
      " |  >>> validation = spark.createDataFrame([(0.0, Vectors.dense(-1.0),)],\n",
      " |  ...              [\"indexed\", \"features\"])\n",
      " |  >>> model.evaluateEachIteration(validation)\n",
      " |  [0.25..., 0.23..., 0.21..., 0.19..., 0.18...]\n",
      " |  >>> model.numClasses\n",
      " |  2\n",
      " |  >>> gbt = gbt.setValidationIndicatorCol(\"validationIndicator\")\n",
      " |  >>> gbt.getValidationIndicatorCol()\n",
      " |  'validationIndicator'\n",
      " |  >>> gbt.getValidationTol()\n",
      " |  0.01\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GBTClassifier\n",
      " |      _JavaProbabilisticClassifier\n",
      " |      ProbabilisticClassifier\n",
      " |      _JavaClassifier\n",
      " |      Classifier\n",
      " |      pyspark.ml.wrapper.JavaPredictor\n",
      " |      pyspark.ml.base.Predictor\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      _ProbabilisticClassifierParams\n",
      " |      pyspark.ml.param.shared.HasProbabilityCol\n",
      " |      pyspark.ml.param.shared.HasThresholds\n",
      " |      _ClassifierParams\n",
      " |      pyspark.ml.param.shared.HasRawPredictionCol\n",
      " |      pyspark.ml.base._PredictorParams\n",
      " |      pyspark.ml.param.shared.HasLabelCol\n",
      " |      pyspark.ml.param.shared.HasFeaturesCol\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      _GBTClassifierParams\n",
      " |      pyspark.ml.tree._GBTParams\n",
      " |      pyspark.ml.tree._TreeEnsembleParams\n",
      " |      pyspark.ml.tree._DecisionTreeParams\n",
      " |      pyspark.ml.param.shared.HasCheckpointInterval\n",
      " |      pyspark.ml.param.shared.HasSeed\n",
      " |      pyspark.ml.param.shared.HasWeightCol\n",
      " |      pyspark.ml.param.shared.HasMaxIter\n",
      " |      pyspark.ml.param.shared.HasStepSize\n",
      " |      pyspark.ml.param.shared.HasValidationIndicatorCol\n",
      " |      pyspark.ml.tree._HasVarianceImpurity\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, lossType: str = 'logistic', maxIter: int = 20, stepSize: float = 0.1, seed: Optional[int] = None, subsamplingRate: float = 1.0, impurity: str = 'variance', featureSubsetStrategy: str = 'all', validationTol: float = 0.01, validationIndicatorCol: Optional[str] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0, weightCol: Optional[str] = None)\n",
      " |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                  maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10,                  lossType=\"logistic\", maxIter=20, stepSize=0.1, seed=None, subsamplingRate=1.0,                  impurity=\"variance\", featureSubsetStrategy=\"all\", validationTol=0.01,                  validationIndicatorCol=None, leafCol=\"\", minWeightFractionPerNode=0.0,                  weightCol=None)\n",
      " |  \n",
      " |  setCacheNodeIds(self, value: bool) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`cacheNodeIds`.\n",
      " |  \n",
      " |  setCheckpointInterval(self, value: int) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`checkpointInterval`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setFeatureSubsetStrategy(self, value: str) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`featureSubsetStrategy`.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  setImpurity(self, value: str) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`impurity`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setLossType(self, value: str) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`lossType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setMaxBins(self, value: int) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`maxBins`.\n",
      " |  \n",
      " |  setMaxDepth(self, value: int) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`maxDepth`.\n",
      " |  \n",
      " |  setMaxIter(self, value: int) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`maxIter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setMaxMemoryInMB(self, value: int) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`maxMemoryInMB`.\n",
      " |  \n",
      " |  setMinInfoGain(self, value: float) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`minInfoGain`.\n",
      " |  \n",
      " |  setMinInstancesPerNode(self, value: int) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`minInstancesPerNode`.\n",
      " |  \n",
      " |  setMinWeightFractionPerNode(self, value: float) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`minWeightFractionPerNode`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, lossType: str = 'logistic', maxIter: int = 20, stepSize: float = 0.1, seed: Optional[int] = None, subsamplingRate: float = 1.0, impurity: str = 'variance', featureSubsetStrategy: str = 'all', validationTol: float = 0.01, validationIndicatorCol: Optional[str] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0, weightCol: Optional[str] = None) -> 'GBTClassifier'\n",
      " |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                   maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10,                   lossType=\"logistic\", maxIter=20, stepSize=0.1, seed=None, subsamplingRate=1.0,                   impurity=\"variance\", featureSubsetStrategy=\"all\", validationTol=0.01,                   validationIndicatorCol=None, leafCol=\"\", minWeightFractionPerNode=0.0,                   weightCol=None)\n",
      " |      Sets params for Gradient Boosted Tree Classification.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setSeed(self, value: int) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`seed`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setStepSize(self, value: int) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`stepSize`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setSubsamplingRate(self, value: float) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`subsamplingRate`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setValidationIndicatorCol(self, value: str) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`validationIndicatorCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setWeightCol(self, value: str) -> 'GBTClassifier'\n",
      " |      Sets the value of :py:attr:`weightCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n",
      " |  \n",
      " |  __orig_bases__ = (pyspark.ml.classification._JavaProbabilisticClassifi...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ProbabilisticClassifier:\n",
      " |  \n",
      " |  setProbabilityCol(self: 'P', value: str) -> 'P'\n",
      " |      Sets the value of :py:attr:`probabilityCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setThresholds(self: 'P', value: List[float]) -> 'P'\n",
      " |      Sets the value of :py:attr:`thresholds`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _JavaClassifier:\n",
      " |  \n",
      " |  setRawPredictionCol(self: 'P', value: str) -> 'P'\n",
      " |      Sets the value of :py:attr:`rawPredictionCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Predictor:\n",
      " |  \n",
      " |  setFeaturesCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`featuresCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setLabelCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`labelCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setPredictionCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param: pyspark.ml.param.Param) -> None\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      params : dict or list or tuple, optional\n",
      " |          an optional param map that overrides embedded params. If a list/tuple of\n",
      " |          param maps is given, this calls fit on each param map and returns a list of\n",
      " |          models.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n",
      " |          fitted model(s)\n",
      " |  \n",
      " |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      paramMaps : :py:class:`collections.abc.Sequence`\n",
      " |          A Sequence of param maps.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`_FitMultipleIterator`\n",
      " |          A thread safe iterable which contains one model for each param map. Each\n",
      " |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |          using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  getProbabilityCol(self) -> str\n",
      " |      Gets the value of probabilityCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n",
      " |  \n",
      " |  getThresholds(self) -> List[float]\n",
      " |      Gets the value of thresholds or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n",
      " |  \n",
      " |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n",
      " |  \n",
      " |  getRawPredictionCol(self) -> str\n",
      " |      Gets the value of rawPredictionCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n",
      " |  \n",
      " |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  getLabelCol(self) -> str\n",
      " |      Gets the value of labelCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  getFeaturesCol(self) -> str\n",
      " |      Gets the value of featuresCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self) -> str\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _GBTClassifierParams:\n",
      " |  \n",
      " |  getLossType(self) -> str\n",
      " |      Gets the value of lossType or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _GBTClassifierParams:\n",
      " |  \n",
      " |  lossType = Param(parent='undefined', name='lossType', doc='...(case-in...\n",
      " |  \n",
      " |  supportedLossTypes = ['logistic']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.tree._GBTParams:\n",
      " |  \n",
      " |  getValidationTol(self) -> float\n",
      " |      Gets the value of validationTol or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.tree._GBTParams:\n",
      " |  \n",
      " |  stepSize = Param(parent='undefined', name='stepSize', doc='...r shrink...\n",
      " |  \n",
      " |  validationTol = Param(parent='undefined', name='validationTol', ...is ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.tree._TreeEnsembleParams:\n",
      " |  \n",
      " |  getFeatureSubsetStrategy(self) -> str\n",
      " |      Gets the value of featureSubsetStrategy or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getSubsamplingRate(self) -> float\n",
      " |      Gets the value of subsamplingRate or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.tree._TreeEnsembleParams:\n",
      " |  \n",
      " |  featureSubsetStrategy = Param(parent='undefined', name='featureSubsetS...\n",
      " |  \n",
      " |  subsamplingRate = Param(parent='undefined', name='subsamplingRate'...r...\n",
      " |  \n",
      " |  supportedFeatureSubsetStrategies = ['auto', 'all', 'onethird', 'sqrt',...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.tree._DecisionTreeParams:\n",
      " |  \n",
      " |  getCacheNodeIds(self) -> bool\n",
      " |      Gets the value of cacheNodeIds or its default value.\n",
      " |  \n",
      " |  getLeafCol(self) -> str\n",
      " |      Gets the value of leafCol or its default value.\n",
      " |  \n",
      " |  getMaxBins(self) -> int\n",
      " |      Gets the value of maxBins or its default value.\n",
      " |  \n",
      " |  getMaxDepth(self) -> int\n",
      " |      Gets the value of maxDepth or its default value.\n",
      " |  \n",
      " |  getMaxMemoryInMB(self) -> int\n",
      " |      Gets the value of maxMemoryInMB or its default value.\n",
      " |  \n",
      " |  getMinInfoGain(self) -> float\n",
      " |      Gets the value of minInfoGain or its default value.\n",
      " |  \n",
      " |  getMinInstancesPerNode(self) -> int\n",
      " |      Gets the value of minInstancesPerNode or its default value.\n",
      " |  \n",
      " |  getMinWeightFractionPerNode(self) -> float\n",
      " |      Gets the value of minWeightFractionPerNode or its default value.\n",
      " |  \n",
      " |  setLeafCol(self: 'P', value: str) -> 'P'\n",
      " |      Sets the value of :py:attr:`leafCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.tree._DecisionTreeParams:\n",
      " |  \n",
      " |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n",
      " |  \n",
      " |  leafCol = Param(parent='undefined', name='leafCol', doc='L...ndex of e...\n",
      " |  \n",
      " |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n",
      " |  \n",
      " |  maxDepth = Param(parent='undefined', name='maxDepth', doc='... node + ...\n",
      " |  \n",
      " |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n",
      " |  \n",
      " |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n",
      " |  \n",
      " |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n",
      " |  \n",
      " |  minWeightFractionPerNode = Param(parent='undefined', name='minWeightFr...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n",
      " |  \n",
      " |  getCheckpointInterval(self) -> int\n",
      " |      Gets the value of checkpointInterval or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n",
      " |  \n",
      " |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  getSeed(self) -> int\n",
      " |      Gets the value of seed or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  getWeightCol(self) -> str\n",
      " |      Gets the value of weightCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  getMaxIter(self) -> int\n",
      " |      Gets the value of maxIter or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasStepSize:\n",
      " |  \n",
      " |  getStepSize(self) -> float\n",
      " |      Gets the value of stepSize or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasValidationIndicatorCol:\n",
      " |  \n",
      " |  getValidationIndicatorCol(self) -> str\n",
      " |      Gets the value of validationIndicatorCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasValidationIndicatorCol:\n",
      " |  \n",
      " |  validationIndicatorCol = Param(parent='undefined', name='validationInd...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.tree._HasVarianceImpurity:\n",
      " |  \n",
      " |  getImpurity(self) -> str\n",
      " |      Gets the value of impurity or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.tree._HasVarianceImpurity:\n",
      " |  \n",
      " |  impurity = Param(parent='undefined', name='impurity', doc='...(case-in...\n",
      " |  \n",
      " |  supportedImpurities = ['variance']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self) -> str\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName: str) -> bool\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self) -> pyspark.ml.util.JavaMLWriter\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path: str) -> None\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path: str) -> ~RL from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fa9c326-f0aa-4c4e-8602-e0afd32af9bc",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fa92e-0041-48d1-bb80-ba5012951bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7866da5a-0f72-4ee7-baf3-6233698931c3",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06274391-57a3-4a78-a443-ed800da78507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distributed-computing",
   "language": "python",
   "name": "distributed-computing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
