{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc445894-a5bd-4768-8672-bcebea5aebfa",
   "metadata": {},
   "source": [
    "### Creating Model 3\n",
    "\n",
    "* Creating a 3rd model for the NFL Play data. The goal is to predict the play type on 4th down.\n",
    "* Logistic Regression & Random Forest have been completed, so the next step is gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1acab1-8e72-4add-9c2c-1798b8967ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import nfl_data_py as nfl\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import pandas as pd\n",
    "from imblearn.combine import SMOTEENN\n",
    "from DataPipelineFxn import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b6116-fab9-4cfd-be1c-8000d1a6292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 done.\n",
      "2001 done.\n",
      "2002 done.\n",
      "2003 done.\n",
      "2004 done.\n",
      "2005 done.\n",
      "2006 done.\n",
      "2007 done.\n",
      "2008 done.\n",
      "2009 done.\n",
      "2010 done.\n",
      "2011 done.\n",
      "2012 done.\n",
      "2013 done.\n",
      "2014 done.\n",
      "2015 done.\n",
      "2016 done.\n",
      "2017 done.\n",
      "2018 done.\n",
      "2019 done.\n",
      "2020 done.\n",
      "2021 done.\n",
      "2022 done.\n",
      "2023 done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/02 15:53:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/02 15:53:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/12/02 15:53:18 WARN TaskSetManager: Stage 0 contains a task of very large size (9437 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/02 15:53:20 WARN TaskSetManager: Stage 3 contains a task of very large size (9437 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/02 15:53:25 WARN TaskSetManager: Stage 4 contains a task of very large size (3174 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/12/02 15:53:28 WARN TaskSetManager: Stage 5 contains a task of very large size (1934 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# get the spark session and data frame\n",
    "spark, df, test_df = GetSparkDF(undersample = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968cfd5-3a1e-4df9-a459-a18f88dfebb4",
   "metadata": {},
   "source": [
    "### Create the Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eaa2e8-7fb1-478e-86eb-ee6fec29cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0fd6a-0322-4cb1-ba56-ab40c1f574de",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_col = [\"home_team\", \"away_team\", \"season_type\", \"posteam\", \"posteam_type\", \"defteam\", \"side_of_field\", \"game_half\",\n",
    "        \"play_type\", \"season\", 'roof', 'surface']\n",
    "str_col_output = [\"home_team_idx\", \"away_team_idx\", \"season_type_idx\", \"posteam_idx\", \"posteam_type_idx\", \"defteam_idx\",\n",
    "                  \"side_of_field_idx\", \"game_half_idx\", \"play_type_idx\", \"season_idx\", 'roof_idx', 'surface_idx']\n",
    "ohe_col_input = [\"home_team_idx\", \"away_team_idx\", \"season_type_idx\", \"posteam_idx\", \"posteam_type_idx\", \"defteam_idx\",\n",
    "                  \"side_of_field_idx\", \"game_half_idx\", \"season_idx\", 'roof_idx', 'surface_idx']\n",
    "ohe_col_vec = [\"home_team_vec\", \"away_team_vec\", \"season_type_vec\", \"posteam_vec\", \"posteam_type_vec\", \"defteam_vec\",\n",
    "                  \"side_of_field_vec\", \"game_half_ivec\", \"season_vec\", 'roof_vec', 'surface_vec']\n",
    "\n",
    "\n",
    "# process using string indexer first for catgeorical features  \n",
    "stringIndexer = StringIndexer(inputCols=str_col, outputCols=str_col_output)\n",
    "\n",
    "# process rating data into second feature\n",
    "ohe = OneHotEncoder(inputCols=ohe_col_input, outputCols=ohe_col_vec) \n",
    "\n",
    "# Assemble features column\n",
    "va = VectorAssembler(inputCols=ohe_col_vec, outputCol=\"features\") \n",
    "\n",
    "# process data using maxabs scaler, not necessarily important for trees but consistency\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# define the model\n",
    "nb = NaiveBayes(\n",
    "    featuresCol = 'features', labelCol = 'play_type_idx'\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline = Pipeline(stages=[stringIndexer, ohe, va, scaler, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23916e45-b255-4d3d-81a7-6ea8711d2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter search across max depth, bins, and iterations\n",
    "# paramGrid = ParamGridBuilder()\\\n",
    "#     .addGrid(gbt.maxDepth, np.arange(10, 100, step = 10)) \\\n",
    "#     .addGrid(gbt.maxIter, [20, 50, 100, 200])\\\n",
    "#     .addGrid(gbt.maxBins, np.arange(10, 100, step = 10))\\\n",
    "#     .build()\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(nb.smoothing, np.arange(0.05, 1, step = 0.05)) \\\n",
    "    .build()\n",
    "\n",
    "# build the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"play_type_idx\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# set up the train validation split\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    evaluator=evaluator,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    # 80% of the data will be used for training, 20% for validation.\n",
    "    trainRatio=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a924059-9309-4b93-b118-caa25f5c0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.roof).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b24073-ddf2-431b-99ac-014484fdfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['roof'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dff5e2-13eb-48ba-bd30-8d7095cf5251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to replace the empty string in surface\n",
    "df = df.withColumn(\n",
    "    'surface',\n",
    "    F.when(df.surface == '', 'Undefined').otherwise(df.surface)\n",
    ")\n",
    "\n",
    "# fit the model with the hyperparameter search\n",
    "nb_model = tvs.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9c326-f0aa-4c4e-8602-e0afd32af9bc",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fa92e-0041-48d1-bb80-ba5012951bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = nb_model.bestModel\n",
    "\n",
    "# Extract the parameter map of the best model\n",
    "best_params = best_model.stages[-1].extractParamMap()\n",
    "\n",
    "# Print the parameters values\n",
    "print(\"Tuned Parameters of the Best Model:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param.name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35437ee-aa32-456f-b6cb-c4f3925686d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to change test df instance\n",
    "test_df = test_df.withColumn(\n",
    "    'surface',\n",
    "    F.when(test_df.surface == '', 'Undefined').otherwise(test_df.surface)\n",
    ")\n",
    "\n",
    "prediction = best_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a9720-fe4d-4c2c-bdd0-02adf1f269e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='play_type_idx', predictionCol=\"prediction\",\n",
    "    metricName='f1'\n",
    ")\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='play_type_idx', predictionCol=\"prediction\",\n",
    "    metricName='accuracy'\n",
    ")\n",
    "\n",
    "precision_by_label_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol='play_type_idx', predictionCol=\"prediction\",\n",
    "    metricName='precisionByLabel'\n",
    ")\n",
    "\n",
    "f1_score = f1_evaluator.evaluate(prediction)\n",
    "accuracy = accuracy_evaluator.evaluate(prediction)\n",
    "precision_by_label = precision_by_label_eval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d502c-6f99-40a5-8c04-13dd353c9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.select([prediction.play_type_idx, prediction.prediction]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93762d04-5930-4435-b6ea-89b4a7410b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.groupby(prediction.prediction).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd6c86-2b93-44f5-b611-f4dec6a3fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.groupby(prediction.play_type_idx).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf18220-4093-4635-ab84-ddfd9284d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = prediction.select([prediction.play_type, prediction.play_type_idx]).distinct().orderBy(prediction.play_type_idx).select(prediction.play_type).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231c8d6-1f67-469b-a6f3-755d82f0ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby(test_df.play_type).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23165c0a-c5b9-42c9-9786-fecd21c7df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = f'''\n",
    "F1: {f1_score}\n",
    "Accuracy: {accuracy}\n",
    "Precision by Label: {precision_by_label}\n",
    "'''\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00f9b4-0009-4294-9d5d-1ab0d2f1adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "#Adapting code from https://www.kaggle.com/code/ashokkumarpalivela/multiclass-classification-using-pyspark for confusion matrix\n",
    "preds_and_labels = prediction.select(['prediction','play_type_idx'])\\\n",
    "                              .withColumn('play_type_idx', col('play_type_idx')\\\n",
    "                              .cast(FloatType()))\\\n",
    "                              .orderBy('prediction')\n",
    "    \n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf723f4-ec66-4858-89ac-098050470fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [\"field_goal\", \"no_play\", \"pass\", \"punt\", \"run\"]\n",
    "_ = plt.figure(figsize=(10, 10))\n",
    "ax = sns.heatmap(metrics.confusionMatrix().toArray(),\n",
    "            cmap='viridis',\n",
    "            annot=True,fmt='0',\n",
    "            cbar=False, \n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels)\n",
    "\n",
    "plt.title('Confusion Matrix', fontsize = 20) # title with fontsize 20\n",
    "plt.xlabel('Predicted', fontsize = 15) # x-axis label with fontsize 15\n",
    "plt.ylabel('Actual', fontsize = 15) # y-axis label with fontsize 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7866da5a-0f72-4ee7-baf3-6233698931c3",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28d1a6-06c7-4524-a4d4-f5fb9f8c2e48",
   "metadata": {},
   "source": [
    "Some quick observations:\n",
    "\n",
    "* Much better performance when we don't undersample, could be because everything is a punt\n",
    "* We need to be careful with the labels for the confusion matrix. They were not intially lining up properly with what the idxs are.\n",
    "* I added temp, wind, roof, surface to the predictors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distributed-computing",
   "language": "python",
   "name": "distributed-computing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
